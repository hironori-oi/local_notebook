# ===========================================
# NotebookLM Local App - Environment Variables
# ===========================================
# Copy this file to .env and update the values
# cp .env.example .env

# ===========================================
# Environment Mode
# ===========================================
# "development" or "production"
# In production mode:
# - JWT_SECRET_KEY validation is strict
# - Swagger docs are disabled
# - Logging level is INFO (not DEBUG)
ENV=development

# Deployment Mode: "local" or "cloud"
# - local: Uses in-memory rate limiter, local file storage
# - cloud: Uses Redis rate limiter, Supabase storage
DEPLOYMENT_MODE=local

# ===========================================
# Database (PostgreSQL with pgvector)
# ===========================================
# For Docker Compose (uses service name):
DATABASE_URL=postgresql://notebooklm:notebooklm_password@postgres:5432/notebooklm

# Docker Compose specific
POSTGRES_USER=notebooklm
POSTGRES_PASSWORD=your_secure_password_here
POSTGRES_DB=notebooklm

# ===========================================
# Port Configuration (Docker Compose)
# ===========================================
# Change these if ports are already in use on your host machine
POSTGRES_PORT=5432
REDIS_PORT=6379
BACKEND_PORT=8000
FRONTEND_PORT=3000
FLOWER_PORT=5555

# ===========================================
# Redis & Celery (Background Task Queue)
# ===========================================
# Redis URL for Celery message broker and result backend
# For Docker Compose: redis://redis:6379/0
# For local development: redis://localhost:6379/0
REDIS_URL=redis://redis:6379/0

# Optional: Separate URLs for broker and backend
# CELERY_BROKER_URL=redis://redis:6379/0
# CELERY_RESULT_BACKEND=redis://redis:6379/1

# Flower monitoring dashboard credentials
FLOWER_USER=admin
FLOWER_PASSWORD=your_flower_password_here

# ===========================================
# LLM Provider Configuration
# ===========================================
# Provider: "ollama" or "vllm"
LLM_PROVIDER=ollama

# Ollama Configuration (native API at /api/chat)
LLM_API_BASE=http://localhost:11434
LLM_MODEL=llama3.1:8b
# Timeout in seconds for LLM requests
LLM_TIMEOUT=120

# For vLLM:
# LLM_API_BASE=http://vllm-server:8000/v1
# LLM_MODEL=your-vllm-model

# ===========================================
# Generation LLM Configuration (Infographic/Slides)
# ===========================================
# Separate model for infographic and slide generation
# If not set, uses LLM_MODEL above
GENERATION_LLM_MODEL=
GENERATION_LLM_MAX_TOKENS=8192

# ===========================================
# Embedding Configuration
# ===========================================
EMBEDDING_API_BASE=http://localhost:11434
EMBEDDING_MODEL=nomic-embed-text
# Embedding dimensions (depends on model)
# nomic-embed-text: 768
# text-embedding-3-small: 1536
EMBEDDING_DIM=768
# Timeout in seconds for embedding requests
EMBEDDING_TIMEOUT=60

# ===========================================
# Authentication
# ===========================================
# IMPORTANT: Generate a secure secret key for production!
# Run: python -c "import secrets; print(secrets.token_hex(32))"
# In production, this MUST be at least 32 characters
JWT_SECRET_KEY=CHANGE_ME_TO_RANDOM_SECRET_KEY

JWT_ALGORITHM=HS256
# Token expiration in minutes (default: 24 hours = 1440 minutes)
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=1440

# ===========================================
# Rate Limiting
# ===========================================
# Authentication endpoints (stricter to prevent brute force)
RATE_LIMIT_AUTH_REQUESTS=5
RATE_LIMIT_AUTH_WINDOW=900

# General API endpoints
RATE_LIMIT_API_REQUESTS=100
RATE_LIMIT_API_WINDOW=60

# ===========================================
# CORS Configuration
# ===========================================
# Comma-separated list of allowed origins
# For production, set to your actual frontend URL
CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000

# ===========================================
# Application Settings
# ===========================================
# Upload directory for source files (for local storage)
UPLOAD_DIR=data/uploads
# Maximum file size in MB
MAX_UPLOAD_SIZE_MB=50

# ===========================================
# Storage Provider (for cloud deployment)
# ===========================================
# Storage provider: "local" or "supabase"
STORAGE_PROVIDER=local

# Supabase Storage Configuration (required if STORAGE_PROVIDER=supabase)
# Get these from your Supabase project dashboard
SUPABASE_URL=
SUPABASE_SERVICE_KEY=
SUPABASE_STORAGE_BUCKET=uploads

# ===========================================
# Frontend Configuration
# ===========================================
# Backend API URL (for frontend to call)
NEXT_PUBLIC_API_BASE_URL=http://localhost:8000

# ===========================================
# Proxy Settings (for deployments behind reverse proxy)
# ===========================================
# Comma-separated list of trusted proxy IPs/networks
# Example: "127.0.0.1,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16"
TRUSTED_PROXIES=
# Whether to trust X-Forwarded-For header (only enable behind trusted proxy)
TRUST_PROXY_HEADERS=false

# ===========================================
# Chat History Settings
# ===========================================
# Maximum number of previous messages to include in LLM context
MAX_CHAT_HISTORY_MESSAGES=20
# Maximum total characters for chat history (to prevent token overflow)
MAX_CHAT_HISTORY_CHARS=8000

# ===========================================
# Content Processing Settings (for email generation)
# ===========================================
# Maximum input text length for formatting (characters)
CONTENT_FORMAT_MAX_LENGTH=20000
# Maximum input text length for summarization (characters)
CONTENT_SUMMARY_MAX_LENGTH=30000
# Maximum characters for fallback content in email generation
CONTENT_FALLBACK_MAX_LENGTH=15000

# ===========================================
# YouTube Transcription (Whisper Server)
# ===========================================
# URL of the external Whisper server for speech-to-text
# The Whisper server should be running on a separate machine with GPU
# See whisper-server/README.md for setup instructions
WHISPER_SERVER_URL=http://192.168.1.100:8001

# Maximum video duration in minutes (longer videos will be rejected)
MAX_VIDEO_DURATION_MINUTES=60

# YouTube Cookies (Base64 encoded) - Required for cloud deployments
# YouTube blocks requests from cloud server IPs. To bypass this:
# 1. Export cookies from your browser using "Get cookies.txt" extension
# 2. Base64 encode the cookies file content
# 3. Set the encoded string here
#
# PowerShell example:
#   $content = Get-Content "youtube.com_cookies.txt" -Raw
#   [Convert]::ToBase64String([System.Text.Encoding]::UTF8.GetBytes($content))
#
# Leave empty for local development (usually not needed)
YOUTUBE_COOKIES_BASE64=
